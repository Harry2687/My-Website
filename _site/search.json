[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Harry Zhong",
    "section": "",
    "text": "About Me\nHello and welcome to my website! I’m Harry, an Actuary (AIAA) working as a Data Analyst.\nI made this website as a way to document various personal projects and to showcase my achievements and plans for the future. This website is still a work in progress, but feel free to look around.\nIf you want to know more about me, feel free to contact me via email or connect with me on LinkedIn. Thanks for visiting!\n\n\n Experience\nEBM Insurance & Risk | Data Intelligence Analyst | Nov 2022 - Present\n\n\n Education\nActuaries Institute | Actuary Program | Jul 2023 - Oct 2023\nCurtin University | Bachelor of Science (Actuarial Science) (Honours) | Feb 2019 - Jun 2023"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Harry Zhong",
    "section": "",
    "text": "Hello and welcome to my website! I’m Harry, an Actuary (AIAA) working as a Data Analyst.\nI made this website as a way to document various personal projects and to showcase my achievements and plans for the future. This website is still a work in progress, but feel free to look around.\nIf you want to know more about me, feel free to contact me via email or connect with me on LinkedIn. Thanks for visiting!"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Harry Zhong",
    "section": "###  Experience",
    "text": "###  Experience\nEBM Insurance & Risk | Data Intelligence Analyst | Nov 2022 - Present"
  },
  {
    "objectID": "goals/index.html",
    "href": "goals/index.html",
    "title": "My Timeline",
    "section": "",
    "text": "Future Goals\n\n\n Past Achievements\n\n2023\n\n\n2022\n\n\nStarted my first graduate job at EBM Insurance & Risk\n\n\n\nGraduated with Bachelor of Science (Actuarial Science) with Distinction\n\n85% Course weighted average.\nObtained all Actuaries Institute Foundation Program exemptions.\nRecipient of the Curtin Excellence Scholarship."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Analysis of My Spotify Data\n\n\n\nK-means\n\n\nR\n\n\n\nCategorisation of my Spotify listening history using k-means clustering.\n\n\n\nHarry Zhong\n\n\nMar 23, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Harry Zhong",
    "section": "###  Education",
    "text": "###  Education\nActuaries Institute | Actuary Program | Jul 2023 - Oct 2023\nCurtin University | Bachelor of Science (Actuarial Science) (Honours) | Feb 2019 - Jun 2023"
  },
  {
    "objectID": "goals/index.html#bachelor-of-science-actuarial-science-with-distinction",
    "href": "goals/index.html#bachelor-of-science-actuarial-science-with-distinction",
    "title": "My Timeline",
    "section": "##### Bachelor of Science (Actuarial Science) with Distinction",
    "text": "##### Bachelor of Science (Actuarial Science) with Distinction\n\nRecipient of the Curtin Excellence Scholarship.\n\n\n2023\n\n\n Future Goals"
  },
  {
    "objectID": "goals/index.html#past-achievements",
    "href": "goals/index.html#past-achievements",
    "title": "My Timeline",
    "section": " Past Achievements",
    "text": "Past Achievements\n\n2023\n\n\nQualified as an Associate Actuary (AIAA).\n\nStudied and passed Asset Liability Management and Communication, Modelling and Professionalism, thus completing the Actuary Program while working full time.\n\n\n\n\nGraduated with Bachelor of Science (Actuarial Science) (Honours).\n\n78% Course weighted average.\nCompleted Data Analytics Principles and Actuarial Control Cycle subjects as part of the Actuaries Institute Actuary Program.\nCompleted my honours dissertation (graded 82%): Comparing Stochastic and Constant Volatility Returns Distributions using the Heston Model.\n\n\n\n\n2022\n\n\nStarted my first graduate job at EBM Insurance & Risk.\n\n\n\n2021\n\n\nGraduated with Bachelor of Science (Actuarial Science) with Distinction.\n\n85% Course weighted average.\nObtained all Actuaries Institute Foundation Program exemptions.\nRecipient of the Curtin Excellence Scholarship."
  },
  {
    "objectID": "timeline/index.html#past-achievements",
    "href": "timeline/index.html#past-achievements",
    "title": "My Timeline",
    "section": " Past Achievements",
    "text": "Past Achievements\n\n2023\n\n\nQualified as an Associate Actuary (AIAA).\n\nStudied and passed Asset Liability Management and Communication, Modelling and Professionalism, thus completing the Actuary Program while working full time.\n\n\n\n\nGraduated with Bachelor of Science (Actuarial Science) (Honours).\n\n78% Course weighted average.\nCompleted Data Analytics Principles and Actuarial Control Cycle subjects as part of the Actuaries Institute Actuary Program.\nCompleted my honours dissertation (graded 82%): Comparing Stochastic and Constant Volatility Returns Distributions using the Heston Model.\n\n\n\n\n2022\n\n\nStarted my first graduate job at EBM Insurance & Risk.\n\n\n\n2021\n\n\nGraduated with Bachelor of Science (Actuarial Science) with Distinction.\n\n85% Course weighted average.\nObtained all Actuaries Institute Foundation Program exemptions.\nRecipient of the Curtin Excellence Scholarship."
  },
  {
    "objectID": "projects/spotify-analysis/index.html",
    "href": "projects/spotify-analysis/index.html",
    "title": "Analysis of My Spotify Data",
    "section": "",
    "text": "The motivation for this project was:\n\nI thought it would be fun.\nThat’s it.\n\nSo, let’s get into how we can use R and Spotify’s Web API to categorise songs that we have listened to."
  },
  {
    "objectID": "projects/spotify-analysis/index.html#spotify-history-data",
    "href": "projects/spotify-analysis/index.html#spotify-history-data",
    "title": "Analysis of My Spotify Data",
    "section": "Spotify History Data",
    "text": "Spotify History Data\n\n\nShow code\nfiles &lt;- list.files(\"data/full_history_data\", \n                    pattern = \"*.json\", \n                    full.names = TRUE)\n\nfull_streaming_history &lt;- foreach(file = files, \n                                  .packages = c(\"jsonlite\"),\n                                  .combine = rbind) %do% {\n  fromJSON(file, flatten = TRUE)\n} %&gt;%\n  rename(track_name = \"master_metadata_track_name\",\n         artist_name = \"master_metadata_album_artist_name\") %&gt;%\n  mutate(track_uri = gsub(\"spotify:track:\", \"\", spotify_track_uri),\n         month = ts %&gt;%\n           substring(1, 7) %&gt;%\n           paste0(\"-01\") %&gt;%\n           ymd()) %&gt;%\n  select(-spotify_track_uri) %&gt;%\n  filter(month &gt;= ymd(\"2019-04-01\"))"
  },
  {
    "objectID": "projects/spotify-analysis/index.html#track-feature-data",
    "href": "projects/spotify-analysis/index.html#track-feature-data",
    "title": "Analysis of My Spotify Data",
    "section": "Track Feature Data",
    "text": "Track Feature Data\nNext, we’ll need to use Spotify’s Web API to obtain track features, which requires the track ID of each track we’re interested in. Fortunately, since we requested our full activity history, this data is included as a column.\nWe can use the httr and jsonlite packages to create a function that takes a Spotify track ID and returns its track features.\n\nget_audio_features &lt;- function(track_id) {\n  url = paste0(\"https://api.spotify.com/v1/audio-features/\", track_id)\n  response &lt;- GET(\n    url,\n    add_headers(Authorization = paste(\"Bearer\", spotify_token))\n  )\n  data &lt;- fromJSON(\n    content(\n      response, \n      \"text\", \n      encoding = \"UTF-8\"\n    )\n  )\n  return(data)\n}\n\nGiven the large number of track IDs, using this function on all tracks in our dataset is a long and painful process, where we will get rate limited many times by Spotify. Conveniently, I have a local file containing all of our tracks and their associated track features, which I will load in.\nThe description for each feature can be found in Spotify’s documentation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrack_name\nartist_name\ntrack_uri\ndanceability\nenergy\nkey\nloudness\nmode\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\nduration_ms\ntime_signature\n\n\n\n\nPsychedelic Switch\nCarly Rae Jepsen\n7zy2kNoeD72x2NEDaAsJOX\n0.681\n0.805\n4\n-6.676\n1\n0.0480\n0.00182\n1.58e-02\n0.341\n0.304\n127.907\n272035\n4\n\n\nSick Feeling\nboy pablo\n7zxLkZbUxITHabPzGN8Xgc\n0.415\n0.504\n9\n-10.003\n1\n0.0318\n0.02200\n3.80e-06\n0.363\n0.401\n165.860\n155714\n4\n\n\nYou Get Me So High\nThe Neighbourhood\n7zwn1eykZtZ5LODrf7c0tS\n0.551\n0.881\n7\n-6.099\n0\n0.0542\n0.18600\n7.91e-02\n0.152\n0.387\n88.036\n153000\n4\n\n\nNo Different\nEpik High\n7ztlf9mCrjoLXAYYf0LCYx\n0.732\n0.600\n1\n-6.127\n1\n0.0535\n0.03070\n8.25e-05\n0.137\n0.238\n131.912\n200362\n4\n\n\nSorry\nThe Rose\n7zmrZMinkTMJ2kZgM9Kqgp\n0.388\n0.642\n10\n-4.659\n1\n0.0337\n0.47100\n0.00e+00\n0.306\n0.402\n173.610\n215477\n4\n\n\nLivin It Up (with Post Malone & A$AP Rocky)\nYoung Thug\n7zjEyeBsaw9gV0jofJLfOM\n0.767\n0.313\n7\n-12.059\n1\n0.0798\n0.83800\n0.00e+00\n0.105\n0.765\n82.582\n210907\n4\n\n\n\n\n\nWe can then remove discrete track features and scale the remaining features so that the clusters are not affected by the difference in magnitude of different features.\n\nfeature_matrix &lt;- full_track_features %&gt;%\n  mutate(track_artist = paste(track_name, artist_name, sep = \" - \")) %&gt;%\n  select(\n    -track_name, \n    -artist_name\n  ) %&gt;%\n  column_to_rownames(var = \"track_artist\") %&gt;%\n  select(\n    -track_uri,\n    -key,\n    -mode,\n    -time_signature\n  ) %&gt;%\n  scale()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndanceability\nenergy\nloudness\nspeechiness\nacousticness\ninstrumentalness\nliveness\nvalence\ntempo\nduration_ms\n\n\n\n\nPsychedelic Switch - Carly Rae Jepsen\n0.5034878\n1.3400006\n0.4206709\n-0.3325940\n-1.3132909\n-0.3305910\n1.3463448\n-0.5950222\n0.3365591\n1.3459625\n\n\nSick Feeling - boy pablo\n-1.2632525\n-0.0712252\n-0.3296591\n-0.5474916\n-1.2506975\n-0.3915509\n1.5205894\n-0.1757695\n1.6071509\n-0.7309358\n\n\nYou Get Me So High - The Neighbourhood\n-0.3599567\n1.6963234\n0.5508003\n-0.2503492\n-0.7420099\n-0.0863063\n-0.1505748\n-0.2362802\n-0.9982436\n-0.7793940\n\n\nNo Different - Epik High\n0.8422237\n0.3788667\n0.5444855\n-0.2596349\n-1.2237122\n-0.3912473\n-0.2693779\n-0.8802870\n0.4706386\n0.0662492\n\n\nSorry - The Rose\n-1.4425833\n0.5757820\n0.8755599\n-0.5222876\n0.1419900\n-0.3915657\n1.0691375\n-0.1714473\n1.8666057\n0.3361258\n\n\nLivin It Up (with Post Malone & A$AP Rocky) - Young Thug\n1.0746895\n-0.9667206\n-0.7933436\n0.0892422\n1.2803337\n-0.3915657\n-0.5228246\n1.3975087\n-1.1808328\n0.2545290"
  },
  {
    "objectID": "projects/spotify-analysis/index.html#what-is-k-means",
    "href": "projects/spotify-analysis/index.html#what-is-k-means",
    "title": "Analysis of My Spotify Data",
    "section": "What is K-means?",
    "text": "What is K-means?\nThe k-means algorithm basically goes:\n\nChoose \\(k\\) random points within the domain of your factors.\nCreate \\(k\\) clusters by assigning each observation to its nearest point, which is now referred to as a mean.\nThe centroid of each cluster then becomes the new mean.\nRepeat until convergence.\n\nThe obvious question is: how do we determine the value of \\(k\\) for a given set of factors? One method would be to use something called a silhouette value. We can understand the silhouette value by considering a group of clustered data points, shown below.\n\n\nChart Code\nset.seed(2687)\n\nrand_data &lt;- data.frame(\n  x = rnorm(50),\n  y = rnorm(50)\n)\n\nkm &lt;- kmeans(rand_data, 3)\n\nfviz_cluster(\n  km,\n  data = rand_data,\n  geom = \"point\"\n)\n\n\n\n\n\n\n\n\n\nWe’ll let \\(s_i\\) be the silhouette value for point \\(i\\) which belongs to cluster \\(C_I\\), then\n\\[\n\\begin{split}\ns_i&=\\frac{b_i-a_i}{\\text{max}(a_i,b_i)},\\text{ if }|C_I|&gt;1,\\\\\ns_i&=0,\\text{ if }|C_I|=0,\n\\end{split}\n\\]\nwhere\n\\[\n\\begin{split}\na_i&=\\frac{1}{|C_I|-1}\\sum_{j\\in C_I,i\\neq j}\\text{d}(i,j),\\\\\nb_i&=\\text{min}\\frac{1}{|C_J|}\\sum_{j\\in C_J}\\text{d}(i,j),\\text{ where }J\\neq I.\n\\end{split}\n\\]\nSimply put, \\(a_i\\) is the average distance between point \\(i\\) and every other point in cluster \\(C_I\\) besides itself, and \\(b_i\\) is the minimum average distance between \\(i\\) and every other point for some other cluster \\(C_J\\). The cluster \\(C_J\\) used to determine \\(b_i\\) is sometimes referred to as the neighboring cluster of point \\(i\\) as it is the next closest cluster after \\(C_I\\)."
  },
  {
    "objectID": "projects/spotify-analysis/index.html#feature-selection",
    "href": "projects/spotify-analysis/index.html#feature-selection",
    "title": "Analysis of My Spotify Data",
    "section": "Feature selection",
    "text": "Feature selection\n\n\nShow code\nkmeans_select_features &lt;- function(n, data, nstart, itermax) {\n  comb_n &lt;- data %&gt;%\n    colnames() %&gt;%\n    combn(n, simplify = FALSE)\n  \n  old_cols &lt;- seq(1, n)\n  new_cols &lt;- paste0(\"factor_\", seq(1, n))\n  \n  new_cols_sym &lt;- syms(new_cols)\n  \n  factor_combinations_n &lt;- do.call(rbind.data.frame, comb_n) %&gt;%\n    rename_with(~new_cols, all_of(old_cols)) %&gt;%\n    mutate(factors = pmap(list(!!!new_cols_sym), c)) %&gt;%\n    mutate(n_factors = n) %&gt;%\n    select(-(!!new_cols)) %&gt;%\n    mutate(data = map(factors,\n                      ~data %&gt;%\n                        as.data.frame() %&gt;%\n                        select(all_of(.x)))) %&gt;%\n    mutate(n_clusters = map(data,\n                            ~fviz_nbclust(.x, \n                                          kmeans, \n                                          nstart = nstart, \n                                          iter.max = itermax)[[\"data\"]] %&gt;%\n                              slice(which.max(y)) %&gt;%\n                              select(clusters) %&gt;%\n                              as.numeric(),\n                            .progress = paste(\"Finding optimal n_clusters:\", \n                                              n, \n                                              \"factors\")) %&gt;%\n             as.numeric()) %&gt;% \n    mutate(km = map2(data,\n                     n_clusters,\n                     ~kmeans(.x, \n                             .y, \n                             nstart = nstart, \n                             iter.max = itermax,\n                             algorithm = \"MacQueen\"),\n                     .progress = paste(\"Calculating kmeans:\", \n                                       n, \n                                       \"factors\"))) %&gt;%\n    mutate(total_withinss = map(km,\n                                ~.x$tot.withinss) %&gt;%\n             as.numeric(),\n           bsstssRatio = map(km,\n                             ~.x$betweenss/.x$totss) %&gt;%\n             as.numeric()) %&gt;%\n    mutate(km_plot = map2(km,\n                          data,\n                          ~fviz_cluster(.x,\n                                        data = .y,\n                                        geom = \"point\",\n                                        ellipse.type = \"convex\"))) %&gt;%\n    arrange(desc(bsstssRatio))  \n  \n  return(factor_combinations_n)\n}\n\n\n\nset.seed(2687)\n\ncl &lt;- makeCluster(detectCores())\nregisterDoParallel(cl)\n\nkmeans_nfact &lt;- foreach(n = seq(2, ncol(feature_matrix)),\n                        .packages = c(\n                          \"tidyverse\",\n                          \"cluster\",\n                          \"factoextra\",\n                          \"rlang\"\n                        ),\n                        .combine = bind_rows) %dopar% {\n                          kmeans_select_features(n, feature_matrix, 25, 1000)\n                        } %&gt;% \n  arrange(desc(bsstssRatio))\n\nstopCluster(cl)\n\n\nload(\"data/R_data/kmeans_nfact.RData\")"
  },
  {
    "objectID": "projects/spotify-analysis/index.html#spotify-activity-history-data",
    "href": "projects/spotify-analysis/index.html#spotify-activity-history-data",
    "title": "Analysis of My Spotify Data",
    "section": "Spotify Activity History Data",
    "text": "Spotify Activity History Data\n\n\nShow code\nfiles &lt;- list.files(\"data/full_history_data\", \n                    pattern = \"*.json\", \n                    full.names = TRUE)\n\nfull_streaming_history &lt;- foreach(file = files, \n                                  .packages = c(\"jsonlite\"),\n                                  .combine = rbind) %do% {\n  fromJSON(file, flatten = TRUE)\n} %&gt;%\n  rename(track_name = \"master_metadata_track_name\",\n         artist_name = \"master_metadata_album_artist_name\") %&gt;%\n  mutate(track_uri = gsub(\"spotify:track:\", \"\", spotify_track_uri),\n         month = ts %&gt;%\n           substring(1, 7) %&gt;%\n           paste0(\"-01\") %&gt;%\n           ymd()) %&gt;%\n  select(-spotify_track_uri) %&gt;%\n  filter(month &gt;= ymd(\"2019-04-01\"))"
  },
  {
    "objectID": "projects/spotify-analysis/index.html#activity-history-data",
    "href": "projects/spotify-analysis/index.html#activity-history-data",
    "title": "Analysis of My Spotify Data",
    "section": "Activity History Data",
    "text": "Activity History Data\nOur Spotify activity history data is given as a set of .json files. We can extract the data from all the .json files into a dataframe and perform some preliminary cleaning using the code below.\n\n\nActivity History Code\nfiles &lt;- list.files(\n  \"data/full_history_data\", \n  pattern = \"*.json\", \n  full.names = TRUE\n)\n\nfull_streaming_history &lt;- foreach(\n  file = files, \n  .packages = c(\"jsonlite\"),\n  .combine = rbind\n) %do% {\n  fromJSON(file, flatten = TRUE)\n} %&gt;%\n  rename(\n    track_name = \"master_metadata_track_name\",\n    artist_name = \"master_metadata_album_artist_name\"\n  ) %&gt;%\n  mutate(\n    track_uri = gsub(\n      \"spotify:track:\", \n      \"\", \n      spotify_track_uri\n    ),\n    month = ts %&gt;%\n      substring(1, 7) %&gt;%\n      paste0(\"-01\") %&gt;%\n      ymd()\n  ) %&gt;%\n  select(\n    -spotify_track_uri,\n    -username,\n    -platform,\n    -ip_addr_decrypted\n  ) %&gt;%\n  filter(month &gt;= ymd(\"2019-04-01\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nts\nms_played\nconn_country\nuser_agent_decrypted\ntrack_name\nartist_name\nmaster_metadata_album_album_name\nepisode_name\nepisode_show_name\nspotify_episode_uri\nreason_start\nreason_end\nshuffle\nskipped\noffline\noffline_timestamp\nincognito_mode\ntrack_uri\nmonth\n\n\n\n\n2024-02-08T23:57:04Z\n2072\nAU\nunknown\nKiss Me More (feat. SZA)\nDoja Cat\nPlanet Her\nNA\nNA\nNA\ntrackdone\nfwdbtn\nFALSE\nTRUE\nFALSE\n1707436622\nFALSE\n3DarAbFujv6eYNliUTyqtz\n2024-02-01\n\n\n2024-02-08T23:57:01Z\n219724\nAU\nunknown\nvampire\nOlivia Rodrigo\nGUTS\nNA\nNA\nNA\nfwdbtn\ntrackdone\nFALSE\nFALSE\nFALSE\n1707436403\nFALSE\n1kuGVB7EU95pJObxwvfwKS\n2024-02-01\n\n\n2024-02-08T23:53:23Z\n2284\nAU\nunknown\nJudas\nLady Gaga\nBorn This Way\nNA\nNA\nNA\nfwdbtn\nfwdbtn\nFALSE\nTRUE\nFALSE\n1707436400\nFALSE\n7F25roCtYi55JouckaayPC\n2024-02-01\n\n\n2024-02-08T23:53:20Z\n1721\nAU\nunknown\nHeads Will Roll\nYeah Yeah Yeahs\nIt’s Blitz!\nNA\nNA\nNA\nfwdbtn\nfwdbtn\nFALSE\nTRUE\nFALSE\n1707436398\nFALSE\n2WRFD9WczJ975X2K1Y9YVs\n2024-02-01\n\n\n2024-02-08T23:53:18Z\n4018\nAU\nunknown\nWhat You Waiting For?\nGwen Stefani\nLove Angel Music Baby\nNA\nNA\nNA\nfwdbtn\nfwdbtn\nFALSE\nTRUE\nFALSE\n1707436394\nFALSE\n0ny5zITdmyNwyTPVzRGscU\n2024-02-01\n\n\n2024-02-08T23:53:14Z\n2404\nAU\nunknown\nSpace Song\nBeach House\nDepression Cherry\nNA\nNA\nNA\nfwdbtn\nfwdbtn\nFALSE\nTRUE\nFALSE\n1707436392\nFALSE\n3CLhX1JkJZ4s5umNnOqCRh\n2024-02-01\n\n\n\n\n\nFrom here, we can use the ggplot2 and shiny packages to visualise trends in my most listened to artists and tracks.\n\n\n\napp.R\nlibrary(shiny)\nlibrary(tidyverse)\nlibrary(here)\nlibrary(foreach)\n\nfiles &lt;- list.files(\n  paste0(here(), \"/data/full_history_data\"), \n  pattern = \"*.json\", \n  full.names = TRUE\n)\n\nfull_streaming_history &lt;- foreach(\n  file = files, \n  .packages = c(\"jsonlite\"),\n  .combine = rbind\n) %do% {\n  fromJSON(file, flatten = TRUE)\n} %&gt;%\n  rename(\n    track_name = \"master_metadata_track_name\",\n    artist_name = \"master_metadata_album_artist_name\"\n  ) %&gt;%\n  mutate(\n    track_uri = gsub(\"spotify:track:\", \"\", spotify_track_uri),\n    month = ts %&gt;%\n      substring(1, 7) %&gt;%\n      paste0(\"-01\") %&gt;%\n      ymd()\n  ) %&gt;%\n  select(-spotify_track_uri) %&gt;%\n  filter(month &gt;= ymd(\"2019-04-01\"))\n\nmin_date &lt;- full_streaming_history %&gt;%\n  pull(month) %&gt;%\n  min()\n\nmax_date &lt;- full_streaming_history %&gt;%\n  pull(month) %&gt;%\n  max()\n\nui &lt;- fluidPage(\n  titlePanel(\"Spotify Streaming History\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\n        \"top_n_artists\",\n        \"Top n Artists\",\n        min = 1,\n        max = 20,\n        value = 10\n      ),\n      sliderInput(\n        \"top_n_tracks\",\n        \"Top n Tracks\",\n        min = 1,\n        max = 20,\n        value = 10\n      ),\n      sliderInput(\n        \"dates\",\n        \"Streaming History Date Range\",\n        min = min_date,\n        max = max_date,\n        value = c(max_date %m-% months(6), max_date)\n      )\n    ),\n    mainPanel(\n      tabsetPanel(\n        type = \"tabs\",\n        tabPanel(\n          \"Artist History Plot\", \n          h2(textOutput(\"artist_history_title\")),\n          plotOutput(\"artist_history_plot\")\n        ),\n        tabPanel(\n          \"Track History Plot\",\n          h2(textOutput(\"track_history_title\")),\n          plotOutput(\"track_history_plot\")\n        )\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  output$artist_history_title &lt;- renderText({\n    top_n_artists &lt;- input$top_n_artists\n    \n    paste0(\n      \"Proportion of Hours Listened: Top \",\n      top_n_artists,\n      \" Artists\"\n    )\n  })\n  \n  output$artist_history_plot &lt;- renderPlot({\n    top_artists &lt;- data.frame(\n      month = full_streaming_history %&gt;%\n        select(month) %&gt;%\n        distinct()\n    ) %&gt;%\n      mutate(\n        top_artists = map(\n          month,\n          ~full_streaming_history %&gt;%\n            filter(month == .x) %&gt;%\n            group_by(artist_name) %&gt;%\n            summarise(time = sum(ms_played)) %&gt;%\n            slice_max(time, n = as.numeric(input$top_n_artists)) %&gt;%\n            pull(artist_name)\n        )\n      )\n    \n    artist_summary &lt;- full_streaming_history %&gt;%\n      filter(\n        month %&gt;%\n          between(input$dates[1], input$dates[2])\n      ) %&gt;%\n      left_join(\n        top_artists,\n        by = \"month\"\n      ) %&gt;%\n      rowwise() %&gt;%\n      filter(artist_name %in% top_artists) %&gt;%\n      group_by(\n        artist_name, \n        month\n      ) %&gt;%\n      summarise(hours_listened = sum(ms_played/(1000*60)))\n    \n    ggplot(artist_summary, aes(x = month, y = hours_listened, fill = artist_name, label = artist_name)) +\n      xlab(\"Date\") +\n      ylab(\"Proportion\") +\n      geom_bar(position = \"fill\", stat = \"identity\") +\n      geom_text(size = 3, position = position_fill(vjust = 0.5)) +\n      theme(legend.position = \"none\")\n  })\n  \n  output$track_history_title &lt;- renderText({\n    top_n_tracks &lt;- input$top_n_tracks\n    \n    paste0(\n      \"Proportion of Hours Listened: Top \",\n      top_n_tracks,\n      \" Tracks\"\n    )\n  })\n  \n  output$track_history_plot &lt;- renderPlot({\n    top_tracks &lt;- data.frame(\n      month = full_streaming_history %&gt;%\n        select(month) %&gt;%\n        distinct()\n    ) %&gt;%\n      mutate(\n        top_tracks = map(\n          month,\n          ~full_streaming_history %&gt;%\n            filter(month == .x) %&gt;%\n            mutate(track_artist_name = paste(track_name, artist_name, sep = \"\\n\")) %&gt;%\n            group_by(track_artist_name) %&gt;%\n            summarise(time = sum(ms_played)) %&gt;%\n            slice_max(time, n = as.numeric(input$top_n_tracks)) %&gt;%\n            pull(track_artist_name)\n        )\n      )\n    \n    track_summary &lt;- full_streaming_history %&gt;%\n      filter(\n        month %&gt;%\n          between(input$dates[1], input$dates[2])\n      ) %&gt;%\n      mutate(track_artist_name = paste(track_name, artist_name, sep = \"\\n\")) %&gt;%\n      left_join(\n        top_tracks,\n        by = \"month\"\n      ) %&gt;%\n      rowwise() %&gt;%\n      filter(track_artist_name %in% top_tracks) %&gt;%\n      group_by(\n        track_artist_name, \n        month\n      ) %&gt;%\n      summarise(hours_listened = sum(ms_played/(1000*60)))\n    \n    ggplot(track_summary, aes(x = month, y = hours_listened, fill = track_artist_name, label = track_artist_name)) +\n      xlab(\"Date\") +\n      ylab(\"Proportion\") +\n      geom_bar(position = \"fill\", stat = \"identity\") +\n      geom_text(size = 3, position = position_fill(vjust = 0.5)) +\n      theme(legend.position = \"none\")\n  })\n}\n\nshinyApp(ui = ui, server = server)"
  }
]