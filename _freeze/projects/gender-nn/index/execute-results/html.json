{
  "hash": "e49b0f74d805aace30dcb59af5b761c8",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Gender Classification using PyTorch\"\ndescription: \"Classification of faces into genders using a convolutional neural network with residual layers.\"\ncategories: [Gender Classification, Neural Networks, Python, PyTorch]\nauthor:\n  name: Harry Zhong\ndate: last-modified\nimage: cnn-layer.png\ndraft: false\nexecute:\n  freeze: auto\n---\n\n# Introduction\n\nNeural networks are cool, they can take complex tasks that are usually pretty easy for humans to do and automate them, given you have sufficient training data and computing power. In this project, we will explore how to make our own neural network, and attempt to predict the gender of faces.\n\n# Neural Networks\n\nTo get a basic understanding of how neural networks, I would recommend watching 3Blue1Brown's [YouTube playlist](https://youtu.be/aircAruvnKk?si=CbemO9CxNtSznhKJ){target=_blank} on neural networks. We will be applying the theory discussed in the playlist in Python using the PyTorch library.\n\n## Sample Data\n\nBefore we define our model architecture, we'll first load some data to enable us to visualise what the model is doing. We can do this using a couple of modules from `torchvision`.\n\n::: {#1433e8c9 .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nimport torchvision\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\n# Some functions we'll need later\nimport modules.functions as func\n```\n:::\n\n\nWe can then use our imported modules to create `Dataset` and `DataLoader` objects. The `Dataset` represents our image data, after applying a transformation which resizes our images to 128 by 128 pixels, converts to grayscale, and then converts the image to a tensor. The `DataLoader` object then creates an iterable object using our `Dataset`, which is useful for accessing our data in batches, this will help us later when we train our model.\n\n::: {#69b920a6 .cell execution_count=2}\n``` {.python .cell-code}\n# Set device for GPU acceleration, if available.\ndevice = func.set_device()\n\nloader = transforms.Compose([\n    transforms.Resize([128, 128]),\n    transforms.Grayscale(1),\n    transforms.ToTensor()\n])\n\nmy_dataset = datasets.ImageFolder(\n    root='test_images/',\n    transform=loader\n)\n\nmy_dataset_loader = DataLoader(\n    my_dataset,\n    batch_size=len(my_dataset),\n    generator=torch.Generator(device=device)\n)\n```\n:::\n\n\nLet's set `images` and `labels` based on the first and only batch in our `DataLoader`.\n\n::: {#bf153a0f .cell execution_count=3}\n``` {.python .cell-code}\ndata = iter(my_dataset_loader)\nimages, labels = next(data)\n```\n:::\n\n\nWe can then access our images, which are currently tensors. We can display the image tensors using a simple wrapper function that uses `matplotlib.pyplot` under the hood.\n\n::: {#b534face .cell execution_count=4}\n``` {.python .cell-code}\nfor i in range(len(images)):\n  func.imshow(images[i])\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=424 height=415}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-2.png){width=424 height=415}\n:::\n:::\n\n\nWe now have Kratos and Freya as tensors! This will be useful later.\n\n## Model Architecture\n\nTo start, we'll need to determine the architecture, or combination of layers, that our neural network will use. I tested model architectures starting from basic multilayer perceptrons to various forms of convolutional neural network (CNN), and found that fairly basic CNNs worked well on training and testing data which are split from the same main data source. However, I found that most CNN architectures failed to generalise well, and had poor accuracy when used on images outside of the training and testing splits.\n\nThe final model I settled on was a form of residual neural network, which adds residual layers to CNNs. We can define the model using `torch`.\n\n::: {#a78f905c .cell execution_count=5}\n``` {.python .cell-code}\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Define recurring sequence of convolution, batch normalisation, and rectified linear activation function layers.\ndef conv_block(in_channels, out_channels, pool=False):\n    layers = [\n        nn.Conv2d(\n            in_channels, \n            out_channels, \n            kernel_size=3, \n            padding=1\n        ),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU()\n    ]\n    if pool:\n        layers.append(\n            nn.MaxPool2d(4)\n        )\n    return nn.Sequential(*layers)\n\nclass resnetModel_128(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        # Define convolution and residual layers based on conv_block function.\n        self.conv_1 = conv_block(1, 64)\n        self.res_1 = nn.Sequential(\n            conv_block(64, 64), \n            conv_block(64, 64)\n        )\n        self.conv_2 = conv_block(64, 256, pool=True)\n        self.res_2 = nn.Sequential(\n            conv_block(256, 256),\n            conv_block(256, 256)\n        )\n        self.conv_3 = conv_block(256, 512, pool=True)\n        self.res_3 = nn.Sequential(\n            conv_block(512, 512),\n            conv_block(512, 512)\n        )\n        self.conv_4 = conv_block(512, 1024, pool=True)\n        self.res_4 = nn.Sequential(\n            conv_block(1024, 1024),\n            conv_block(1024, 1024)\n        )\n\n        # Define classifier function using fully connected, dropout, and rectified linear activation function.\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(2*2*1024, 2048),\n            nn.Dropout(0.5),\n            nn.ReLU(),\n            nn.Linear(2048, 1024),\n            nn.Dropout(0.5),\n            nn.ReLU(),\n            nn.Linear(1024, 2)\n        )\n    \n    # Define forward function using functions initialised earlier, which outputs predictions.\n    def forward(self, x):\n        x = self.conv_1(x)\n        x = self.res_1(x) + x\n        x = self.conv_2(x)\n        x = self.res_2(x) + x\n        x = self.conv_3(x)\n        x = self.res_3(x) + x\n        x = self.conv_4(x)\n        x = self.res_4(x) + x\n        x = self.classifier(x)\n        x = F.softmax(x, dim=1)\n        return x\n```\n:::\n\n\nNow let's create an instance of `resnetModel_128` and define our classes.\n\n::: {#7ba39862 .cell execution_count=6}\n``` {.python .cell-code}\n# Set seed for reproducibility.\ntorch.manual_seed(2687)\nresnet = resnetModel_128()\nclasses = ('Female', 'Male')\n```\n:::\n\n\nWe now have `resnet` which is our model which we defined earlier, but with completely random parameters. Let's make a prediction based on the untrained model\n\n::: {#0a935d05 .cell execution_count=7}\n``` {.python .cell-code}\nresnet.eval()\nwith torch.no_grad():\n  output = resnet.forward(images.to(device))\n  predicted = torch.max(output.data, 1)[1]\n\nfor i in range(len(predicted)):\n  print(f'Image: {my_dataset.imgs[i][0]}')\n  print(f'Prediction: {classes[predicted[i]]}')\n  print(f'Actual: {classes[labels[i]]}')\n  print(f'{classes[0]} weight: {output[i][0]}')\n  print(f'{classes[1]} weight: {output[i][1]}\\n')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nImage: test_images/Female/freya.png\nPrediction: Female\nActual: Female\nFemale weight: 0.5020168423652649\nMale weight: 0.4979831576347351\n\nImage: test_images/Male/kratos.png\nPrediction: Female\nActual: Male\nFemale weight: 0.5018221139907837\nMale weight: 0.4981779158115387\n\n```\n:::\n:::\n\n\nAs expected, the model is doing nothing more than randomly guessing. Next, we'll explore how we can train our model and make it smarter.\n\n# Training\n\nIn order to make our model better, we need to train it. Training a neural network requires a training dataset and goes through the following iteration until the model is sufficiently trained.\n\n1. Make predictions on training data using forward propagation.\n2. Calculate loss measure based on the difference between predicted labels and actual labels.\n3. Use back propagation to determine changes to parameters required to minimise loss measure.\n4. Make changes to parameters.\n\nLet's explore how we can do this using PyTorch.\n\n## Dataset\n\nConveniently for us, the [CelebA](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html){target=_blank} is a publicly available labelled dataset of around 200k faces. As it's a pretty well known dataset, there is a function in PyTorch that creates a automatically creates a dataset object for CelebA.\n\n::: {#5af199e3 .cell execution_count=8}\n``` {.python .cell-code}\nimsize = int(128/0.8)\nbatch_size = 10\n\nfivecrop_transform = transforms.Compose([\n    transforms.Resize([imsize, imsize]),\n    transforms.Grayscale(1),\n    transforms.FiveCrop(int(imsize*0.8)),\n    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n    transforms.Normalize(0, 1)\n])\n\ntrain_dataset = datasets.CelebA(\n    root = './',\n    split='all',\n    target_type='attr',\n    transform=fivecrop_transform,\n    download=True\n)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    generator=torch.Generator(device=device)\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFiles already downloaded and verified\n```\n:::\n:::\n\n\n::: {#c1e13b27 .cell execution_count=9}\n``` {.python .cell-code}\ntrain_data = iter(train_loader)\ntrain_images, train_labels = next(train_data)\n\nfactor = func.attributes.index('Male')\n```\n:::\n\n\n::: {#a0734b65 .cell execution_count=10}\n``` {.python .cell-code}\nfunc.imshow(torchvision.utils.make_grid(train_images[0]))\nprint('Selected factor:')\nprint(classes[train_labels[:, factor][0]])\nprint('\\nAll factors:')\nfor i, value in enumerate(train_labels[0]):\n    if value.item() == 1:\n        print(func.attributes[i])\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){width=575 height=152}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nSelected factor:\nMale\n\nAll factors:\nArched_Eyebrows\nBags_Under_Eyes\nBig_Lips\nBig_Nose\nBushy_Eyebrows\nGoatee\nMale\nMustache\nWearing_Hat\nWearing_Necklace\nYoung\n```\n:::\n:::\n\n\n::: {#11c4d248 .cell execution_count=11}\n``` {.python .cell-code}\nresnet.train()\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(\n    resnet.parameters(), \n    lr=0.01,\n    momentum=0.9,\n    weight_decay=0.001\n)\nscheduler = torch.optim.lr_scheduler.StepLR(\n    optimizer=optimizer,\n    step_size=1,\n    gamma=0.1\n)\n```\n:::\n\n\n::: {#4887b73c .cell execution_count=12}\n``` {.python .cell-code}\nparameters = func.n_parameters(resnet)\nprint(f'Total Parameters: {parameters[0]}')\nprint(f'Trainable Parameters: {parameters[1]}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTotal Parameters: 41400194\nTrainable Parameters: 41400194\n```\n:::\n:::\n\n\n::: {#fcacb8e3 .cell execution_count=13}\n``` {.python .cell-code}\nepochs = 2\ntrain_losses = []\ntest_losses = []\nfor i in range(epochs):\n    train_cr = 0\n    test_cr = 0\n\n    for j, (X_train, y_train) in enumerate(train_loader):\n        X_train = X_train.to(device)\n        y_train = y_train[:, factor]\n\n        bs, ncrops, c, h, w = X_train.size()\n        y_pred_crops = resnet.forward(X_train.view(-1, c, h, w))\n        y_pred = y_pred_crops.view(bs, ncrops, -1).mean(1)\n\n        loss = criterion(y_pred, y_train)\n\n        predicted = torch.max(y_pred.data, 1)[1]\n        train_batch_cr = (predicted == y_train).sum()\n        train_cr += train_batch_cr\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if (j+1) % 10 == 0:\n            print(f'\\nEpoch: {i+1} | Train Batch: {j+1}')\n            print(f'Train Loss: {loss.item()}')\n            print(f'Train Accuracy: {train_batch_cr/len(X_train)}')\n\n    train_losses.append(loss.item())\n    train_correct.append(train_cr.item())\n\n    scheduler.step()\n```\n:::\n\n\n## Stochastic Gradient Descent\n\n# Results\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}