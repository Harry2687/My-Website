{
  "hash": "a50731af533cc21fad680f6f6bbffcfa",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Gender Classification using PyTorch\"\ndescription: \"Classification of faces into genders using a convolutional neural network with residual layers.\"\ncategories: [Gender Classification, Neural Networks, Python, PyTorch]\nauthor:\n  name: Harry Zhong\ndate: 2024-05-24\nimage: cnn-layer.png\n---\n\n# Introduction\n\nNeural networks are cool, they can take complex tasks that are usually pretty easy for humans to do and automate them, given you have sufficient training data and computing power. In this project, we will explore how to make our own neural network, and attempt to predict the gender of faces.\n\nTo get a basic understanding of how neural networks, I would recommend watching 3Blue1Brown's [YouTube playlist](https://youtu.be/aircAruvnKk?si=CbemO9CxNtSznhKJ){target=_blank} on neural networks. As neural networks are slightly more complicated than most common machine learning algorithms, I won't go through the basics in much detail here.\n\n# Objective\n\nFor this project, we'll aim to successfully classify the following images of Freya, Kratos, and me using a neural network model. \n\n::: {layout-ncol=3}\n![Freya](test_images/Female/freya.png)\n\n![Kratos](test_images/Male/kratos.png)\n\n![Me](test_images/Male/me.png)\n:::\n\nWe'll achieve this using publicly available training and testing datasets. But first, we need to load these images as a data type that can be inputted into a PyTorch neural network, a `torch` tensor. We can do this using a couple of modules from `torchvision`.\n\n::: {#50501492 .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\nimport torchvision\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\n# Some functions we'll need later\nimport modules.functions as functions\n```\n:::\n\n\nWe can then use our imported modules to create `Dataset` and `DataLoader` objects. The `Dataset` represents our image data, after applying a transformation which resizes our images to 128 by 128 pixels, converts to grayscale (this saves us some computational power, hopefully colour isn't an important feature), and then converts the image to a tensor. The `DataLoader` object then creates an iterable object using our `Dataset`, which is useful for accessing our data in batches, this will help us later when we train our model.\n\n::: {#5beb208a .cell execution_count=2}\n``` {.python .cell-code}\n# Set device for GPU acceleration, if available.\ndevice = functions.set_device()\n\nloader = transforms.Compose([\n    transforms.Resize([128, 128]),\n    transforms.Grayscale(1),\n    transforms.ToTensor()\n])\n\nmy_dataset = datasets.ImageFolder(\n    root='test_images/',\n    transform=loader\n)\n\nmy_dataset_loader = DataLoader(\n    my_dataset,\n    batch_size=len(my_dataset),\n    generator=torch.Generator(device=device)\n)\n```\n:::\n\n\nLet's set `images` and `labels` as the image and label tensors in the first and only batch in our `DataLoader`.\n\n::: {#1eef7ae8 .cell execution_count=3}\n``` {.python .cell-code}\ndata = iter(my_dataset_loader)\nimages, labels = next(data)\n```\n:::\n\n\nWe can then display the image tensors using a simple function that uses `matplotlib.pyplot` under the hood.\n\n::: {#a0e823f8 .cell execution_count=4}\n``` {.python .cell-code}\nfunctions.imshow(torchvision.utils.make_grid(images))\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=575 height=221}\n:::\n:::\n\n\n# Datasets\n\nAs with any supervised machine learning algorithm, neural networks require a training and testing dataset for the model to learn and evaluate out of sample performance. In this section we'll explore what this looks like for a neural network.\n\n## Training\n\nFor this project, we'll require a dataset containing a large number of labelled images of faces, which as you can imagine isn't all that common. Luckily for us, the [CelebA](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html){target=_blank} is a publicly available labelled dataset of around 200k faces. As it's a well known dataset, there is a function in `torch` that automatically downloads the required files (sometimes, usually the Google drive link is down) and creates a dataset object for the CelebA.\n\n::: {#b5afbb20 .cell execution_count=5}\n``` {.python .cell-code}\nimsize = int(128/0.8)\nbatch_size = 10\nclasses = ('Female', 'Male')\n\nfivecrop_transform = transforms.Compose([\n    transforms.Resize([imsize, imsize]),\n    transforms.Grayscale(1),\n    transforms.FiveCrop(int(imsize*0.8)),\n    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))\n])\n\ntrain_dataset = datasets.CelebA(\n    root = './',\n    split='all',\n    target_type='attr',\n    transform=fivecrop_transform,\n    download=True\n)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    generator=torch.Generator(device=device)\n)\n```\n:::\n\n\nWe can verify the number of training images using `len`.\n\n::: {#2d2d8b3e .cell execution_count=6}\n``` {.python .cell-code}\nlen(train_dataset)\n```\n\n::: {.cell-output .cell-output-display execution_count=35}\n```\n202599\n```\n:::\n:::\n\n\nNote that the set of transformations applied to the training dataset contains `FiveCrop` in addition to the standard resize and grayscale transformations, `FiveCrop` makes 5 cropped versions of each image (who would have guessed), one for each corner plus centered. This improves model performance and reduces overfitting to the training dataset. However, this also increases the computational resources required to train the model on this dataset by a factor of 5.\n\n::: {.callout-note}\nThere is also a `TenCrop` function which applies the transformations from `FiveCrop`, plus a vertical flip. I would have liked to use `TenCrop`, but my old MacBook did not agree with that decision.\n:::\n\nWe can then access a few sample training images and their labels as we did previously.\n\n::: {#fba523eb .cell execution_count=7}\n``` {.python .cell-code}\ntrain_data = iter(train_loader)\ntrain_images, train_labels = next(train_data)\n\n# Index of Male label, as CelebA contains multiple labels.\nfactor = functions.attributes.index('Male')\n\nfunctions.imshow(torchvision.utils.make_grid(\n    torch.cat((\n        train_images[0],\n        train_images[1],\n        train_images[2]\n    )),\n    nrow=5\n))\n\nfor i in range(3):\n    print(classes[train_labels[:, factor][i]])\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=575 height=359}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMale\nMale\nFemale\n```\n:::\n:::\n\n\n## Testing\n\nNext, we need a dataset to test the performance of our model on unseen data. The simple option would be to split CelebA into train and test partitions. However, I found achieving high test accuracy under this setup to be fairly simple, and resulted in poor performance on other image datasets.\n\nThus, we'll use a [Kaggle dataset](https://www.kaggle.com/datasets/bwandowando/all-these-people-dont-exist){target=_blank} of AI generated faces as the test dataset, which I found required a significantly more complicated model to achieve high accuracy in, but produced models with better performance when given a random selection of my own images.\n\n::: {#878c9991 .cell execution_count=8}\n``` {.python .cell-code}\ntest_transform = transforms.Compose([\n    transforms.Resize([int(imsize*0.8), int(imsize*0.8)]),\n    transforms.Grayscale(1),\n    transforms.ToTensor()\n])\n\ntest_dataset = datasets.ImageFolder(\n    root='ThisPersonDoesNotExist_resize/',\n    transform=test_transform\n)\n\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    generator=torch.Generator(device=device)\n)\n```\n:::\n\n\nOnce again, we can get the number of images in the test dataset.\n\n::: {#3e9538f4 .cell execution_count=9}\n``` {.python .cell-code}\nlen(test_dataset)\n```\n\n::: {.cell-output .cell-output-display execution_count=38}\n```\n6873\n```\n:::\n:::\n\n\n::: {.callout-note}\nThis dataset was originally the training dataset, given significantly reduced number of images compared to CelebA, it's unsurprising the initial models did not perform well.\n:::\n\nWe can then show a few images from the test dataset, along with their labels.\n\n::: {#14e4c2a4 .cell execution_count=10}\n``` {.python .cell-code}\ntest_data = iter(test_loader)\ntest_images, test_labels = next(test_data)\n\nfunctions.imshow(torchvision.utils.make_grid(test_images, nrow=5))\n\nfor i in range(batch_size):\n    print(classes[test_labels[i]])\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){width=575 height=255}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nFemale\nFemale\nMale\nFemale\nFemale\nFemale\nFemale\nFemale\nFemale\nMale\n```\n:::\n:::\n\n\n# Model Architecture\n\nNext, we'll need to determine the architecture, or combination of layers and activation functions, that our neural network will use. I'll skip the experimentation and failed models part of this project but I found that a scaled up version of the model used in this [repository](https://github.com/CallanL/Facial_Feature_Detection_Neural_Network/tree/main){target=_blank} by [CallenL](https://github.com/CallanL){target=_blank} worked the best (of the models I tried). This model seemed to perform better due to a combination of having residual layers (enabling back propogation to work better) and more convolution layers (allowing more features to be detected).\n\n::: {#cff88021 .cell execution_count=11}\n``` {.python .cell-code}\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Define recurring sequence of convolution, batch normalisation, and rectified linear activation function layers.\ndef conv_block(in_channels, out_channels, pool=False):\n    layers = [\n        nn.Conv2d(\n            in_channels, \n            out_channels, \n            kernel_size=3, \n            padding=1\n        ),\n        nn.BatchNorm2d(out_channels),\n        nn.ReLU()\n    ]\n    if pool:\n        layers.append(\n            nn.MaxPool2d(4)\n        )\n    return nn.Sequential(*layers)\n\nclass resnetModel_128(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        # Define convolution and residual layers based on conv_block function.\n        self.conv_1 = conv_block(1, 64)\n        self.res_1 = nn.Sequential(\n            conv_block(64, 64), \n            conv_block(64, 64)\n        )\n        self.conv_2 = conv_block(64, 256, pool=True)\n        self.res_2 = nn.Sequential(\n            conv_block(256, 256),\n            conv_block(256, 256)\n        )\n        self.conv_3 = conv_block(256, 512, pool=True)\n        self.res_3 = nn.Sequential(\n            conv_block(512, 512),\n            conv_block(512, 512)\n        )\n        self.conv_4 = conv_block(512, 1024, pool=True)\n        self.res_4 = nn.Sequential(\n            conv_block(1024, 1024),\n            conv_block(1024, 1024)\n        )\n\n        # Define classifier function using fully connected, dropout, and rectified linear activation function.\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(2*2*1024, 2048),\n            nn.Dropout(0.5),\n            nn.ReLU(),\n            nn.Linear(2048, 1024),\n            nn.Dropout(0.5),\n            nn.ReLU(),\n            nn.Linear(1024, 2)\n        )\n    \n    # Define forward function using functions initialised earlier, which outputs predictions.\n    def forward(self, x):\n        x = self.conv_1(x)\n        x = self.res_1(x) + x\n        x = self.conv_2(x)\n        x = self.res_2(x) + x\n        x = self.conv_3(x)\n        x = self.res_3(x) + x\n        x = self.conv_4(x)\n        x = self.res_4(x) + x\n        x = self.classifier(x)\n        x = F.softmax(x, dim=1)\n        return x\n```\n:::\n\n\nWe can now create a variable using our neural network class.\n\n::: {#5fc11b55 .cell execution_count=12}\n``` {.python .cell-code}\n# Set seed for reproducibility.\ntorch.manual_seed(2687)\nresnet = resnetModel_128()\n```\n:::\n\n\nNow is also a good time to check how many parameters (individual weights and biases) our model contains.\n\n::: {#c6b32770 .cell execution_count=13}\n``` {.python .cell-code}\ntotal_params, trainable_params = functions.n_parameters(resnet)\nprint(f'Total Parameters: {total_params}')\nprint(f'Trainable Parameters: {trainable_params}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTotal Parameters: 41400194\nTrainable Parameters: 41400194\n```\n:::\n:::\n\n\nThe variable `resnet` is our model initialised with completely random parameters. For fun, let's make a prediction based on the untrained model.\n\n::: {#0375a901 .cell execution_count=14}\n``` {.python .cell-code}\nresnet.eval()\nwith torch.no_grad():\n  output = resnet.forward(images.to(device))\n  predicted = torch.max(output.data, 1)[1]\n\nfor i in range(len(predicted)):\n  print(f'Image: {my_dataset.imgs[i][0]}')\n  print(f'Prediction: {classes[predicted[i]]}')\n  print(f'Actual: {classes[labels[i]]}')\n  print(f'{classes[0]} weight: {output[i][0]}')\n  print(f'{classes[1]} weight: {output[i][1]}\\n')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nImage: test_images/Female/freya.png\nPrediction: Female\nActual: Female\nFemale weight: 0.5020168423652649\nMale weight: 0.4979831576347351\n\nImage: test_images/Male/kratos.png\nPrediction: Female\nActual: Male\nFemale weight: 0.5018221139907837\nMale weight: 0.4981779158115387\n\nImage: test_images/Male/me.png\nPrediction: Female\nActual: Male\nFemale weight: 0.5015270113945007\nMale weight: 0.49847298860549927\n\n```\n:::\n:::\n\n\nAs expected, the weights are about 50-50 which indicates the model isn't doing much predicting.\n\n# Training\n\nSo, how to we change the parameters of the model such that it generates more accurate outputs? Basically, by doing an interative process known as backpropagation, which incrementally changes the model parameters based on the partial derivatives of the parameters with respect to the loss function, which minimises error and thus makes the model more accurate. This [YouTube video](https://youtu.be/SmZmBKc7Lrs?si=wrAQCV6b65fKmeao){target=_blank} by Artem Kirsanov provides a more detailed explanation of backpropagation.\n\nIn the code below, `criterion` specifies the loss function and `optimizer` specifies the optimisation algorithm used, which in this case is stochastic gradient descent. The additional optional variable `scheduler` specifies how the learning rate changes. Here I have used `torch.optim.lr_scheduler.StepLR` to multiply the learning rate by `0.1` after every step, with step being defined as an epoch in the training loop.\n\n::: {#23603c82 .cell execution_count=15}\n``` {.python .cell-code}\nresnet.train()\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(\n    resnet.parameters(),\n    lr=0.01,\n    momentum=0.9,\n    weight_decay=0.001\n)\nscheduler = torch.optim.lr_scheduler.StepLR(\n    optimizer=optimizer,\n    step_size=1,\n    gamma=0.1\n)\n```\n:::\n\n\nNow we can train our model. For each batch in our training data, we need to:\n\n1. Resize the input tensors such that `resnet.forward` can take all cropped image tensors as inputs.\n2. Average the outputs of each group of cropped image tensors, so each distinct image only gets one final prediction.\n3. Calculate loss based on predicted and actual labels.\n4. Update parameters using backpropagation.\n5. Record loss and accuracy.\n\n::: {.callout-note}\nSteps 1 and 2 would be unnecessary if we didn't use `FiveCrop`.\n:::\n\nWhen we have completed this loop for the entire training dataset, we then make predictions on the entire test dataset via its batches and record the loss and accuracy for each batch.\n\nThis entire process is called an epoch, and we specify how many epochs to train for. I chose 2 epochs for this model due to time constraints (this exact training setup took 20 hours on my 2020 MacBook Pro), and because it resulted in satisfactory performance anyway.\n\n::: {#087ddac7 .cell execution_count=16}\n``` {.python .cell-code}\nepochs = 2\ntrain_losses = []\ntest_losses = []\ntrain_accuracy = []\ntest_accuracy = []\nfor i in range(epochs):\n    epoch_time = 0\n\n    for j, (X_train, y_train) in enumerate(train_loader):\n        X_train = X_train.to(device)\n        y_train = y_train[:, factor]\n\n        # Input all crops as separate images.\n        bs, ncrops, c, h, w = X_train.size()\n        y_pred_crops = resnet.forward(X_train.view(-1, c, h, w))\n        # Let image prediction be the mean of crop predictions.\n        y_pred = y_pred_crops.view(bs, ncrops, -1).mean(1)\n\n        loss = criterion(y_pred, y_train)\n\n        predicted = torch.max(y_pred.data, 1)[1]\n        train_batch_accuracy = (predicted == y_train).sum()/len(X_train)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        train_losses.append(loss.item())\n        train_accuracy.append(train_batch_accuracy.item())\n        \n        print(f'\\nEpoch: {i+1}/{epochs} | Train Batch: {j+1}/{len(train_loader)}')\n        print(f'Train Loss: {loss}')\n        print(f'Train Accuracy: {train_batch_accuracy}')\n        break\n\n    with torch.no_grad():\n        for j, (X_test, y_test) in enumerate(test_loader):\n            X_test = X_test.to(device)\n            y_val = resnet.forward(X_test)\n\n            loss = criterion(y_val, y_test)\n\n            predicted = torch.max(y_val.data, 1)[1]\n            test_batch_accuracy = (predicted == y_test).sum()/len(X_test)\n\n            test_losses.append(loss.item())\n            test_accuracy.append(test_batch_accuracy.item())\n\n            print(f'\\nEpoch: {i+1}/{epochs} | Test Batch: {j+1}/{len(test_loader)}')\n            print(f'Test Loss: {loss}')\n            print(f'Test Accuracy: {test_batch_accuracy}')\n            break\n\n    scheduler.step()\n    break\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nEpoch: 1/2 | Train Batch: 1/20260\nTrain Loss: 0.7186497449874878\nTrain Accuracy: 0.5\n\nEpoch: 1/2 | Test Batch: 1/688\nTest Loss: 0.746019721031189\nTest Accuracy: 0.30000001192092896\n```\n:::\n:::\n\n\nSince the training loop takes a significant amount of time, I've broken the loop to demonstrate how it works on this page. However, the loss and accuracy data produced by the training loop was conveniently saved last time the model was trained, so we'll load it in with `pandas` and plot it.\n\n::: {#62d96223 .cell execution_count=17}\n``` {.python .cell-code}\nimport pandas as pd\n\ntrain_plot_data = pd.read_csv('training_plot_data/train_data.csv')\ntest_plot_data = pd.read_csv('training_plot_data/test_data.csv')\n\ntrain_plot_data = train_plot_data.drop(columns='Unnamed: 0')\ntest_plot_data = test_plot_data.drop(columns='Unnamed: 0')\n\ntrain_plot_MA_data = train_plot_data.rolling(500).mean()\ntrain_plot_MA_data.dropna(inplace=True)\n\ntest_plot_MA_data = test_plot_data.rolling(50).mean()\ntest_plot_MA_data.dropna(inplace=True)\n```\n:::\n\n\n::: {.panel-tabset}\n\n## Training Plot\n\n::: {#99f2a1ee .cell execution_count=18}\n``` {.python .cell-code}\ntrain_plot_MA_data.plot(\n    use_index=True, \n    title='Training Loss/Accuracy (2 Epochs)', \n    xlabel='Batch'\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-19-output-1.png){width=571 height=449}\n:::\n:::\n\n\n## Testing Plot\n\n::: {#5e1341e0 .cell execution_count=19}\n``` {.python .cell-code}\ntest_plot_MA_data.plot(\n    use_index=True, \n    title='Test Loss/Accuracy (2 Epochs)', \n    xlabel='Batch'\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-20-output-1.png){width=573 height=449}\n:::\n:::\n\n\n:::\n\nWe can notice that the model converges fairly quickly, with improvements to performance plateauing after about 5000 batches. Testing performance stays relatively consistent, as it's only evaluated after a full epoch of training is completed, and the model converges before a full epoch is complete.\n\n# Results\n\nGoing back to our original objective, we will once again attempt to classify Freya, Kratos, and me. But this time, we will use our trained model parameters, which can be loaded in using `torch.load`.\n\n::: {#07d53059 .cell execution_count=20}\n``` {.python .cell-code}\nresnet.load_state_dict(torch.load('trained_models/resnetModel_128_epoch_2.pt', map_location=device))\n\nresnet.eval()\nwith torch.no_grad():\n  output = resnet.forward(images.to(device))\n  predicted = torch.max(output.data, 1)[1]\n\nfor i in range(len(predicted)):\n  print(f'Image: {my_dataset.imgs[i][0]}')\n  print(f'Prediction: {classes[predicted[i]]}')\n  print(f'Actual: {classes[labels[i]]}')\n  print(f'{classes[0]} weight: {output[i][0]}')\n  print(f'{classes[1]} weight: {output[i][1]}\\n')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nImage: test_images/Female/freya.png\nPrediction: Female\nActual: Female\nFemale weight: 0.9999755620956421\nMale weight: 2.4402881535934284e-05\n\nImage: test_images/Male/kratos.png\nPrediction: Male\nActual: Male\nFemale weight: 0.019050072878599167\nMale weight: 0.9809499382972717\n\nImage: test_images/Male/me.png\nPrediction: Male\nActual: Male\nFemale weight: 0.19523011147975922\nMale weight: 0.8047698736190796\n\n```\n:::\n:::\n\n\nSuccess! The neural network successfully classified all 3 images. It even picked up that Kratos has a higher likelihood of being a man compared to me, which is fairly reasonable.\n\nTo get a bit of insight into what the neural network is doing we can visualise what the happens to an image as it passes through the convolutional layers. This shows us how the convolutional layers chooses features and the weight of each feature for our chosen image.\n\n::: {#6c7061b3 .cell execution_count=21}\n``` {.python .cell-code}\n# Choose image of Kratos.\nimage = images[1]\n\nwith torch.no_grad():\n    layer_1 = resnet.conv_1(image.to(device).unsqueeze(0))\n    layer_2 = resnet.res_1(layer_1) + layer_1\n    layer_3 = resnet.conv_2(layer_2)\n    layer_4 = resnet.res_2(layer_3) + layer_3\n    layer_5 = resnet.conv_3(layer_4)\n    layer_6 = resnet.res_3(layer_5) + layer_5\n    layer_7 = resnet.conv_4(layer_6)\n    layer_8 = resnet.res_4(layer_7) + layer_7\n```\n:::\n\n\n::: {.panel-tabset}\n\n## Layer 1\n\n::: {#bf38d156 .cell execution_count=22}\n``` {.python .cell-code}\nfunctions.imshow(\n    torchvision.utils.make_grid(\n        layer_1.squeeze(0).unsqueeze(1),\n        nrow=int(64**0.5)\n    )\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-23-output-1.png){width=434 height=416}\n:::\n:::\n\n\n## Layer 2\n\n::: {#9889a59f .cell execution_count=23}\n``` {.python .cell-code}\nfunctions.imshow(\n    torchvision.utils.make_grid(\n        layer_2.squeeze(0).unsqueeze(1),\n        nrow=int(64**0.5)\n    )\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-24-output-1.png){width=434 height=416}\n:::\n:::\n\n\n## Layer 3\n\n::: {#167d82a6 .cell execution_count=24}\n``` {.python .cell-code}\nfunctions.imshow(\n    torchvision.utils.make_grid(\n        layer_3.squeeze(0).unsqueeze(1),\n        nrow=int(256**0.5)\n    )\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-25-output-1.png){width=424 height=416}\n:::\n:::\n\n\n## Layer 4\n\n::: {#1ad300b1 .cell execution_count=25}\n``` {.python .cell-code}\nfunctions.imshow(\n    torchvision.utils.make_grid(\n        layer_4.squeeze(0).unsqueeze(1),\n        nrow=int(256**0.5)\n    )\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-26-output-1.png){width=424 height=416}\n:::\n:::\n\n\n## Layer 5\n\n::: {#48b87e4c .cell execution_count=26}\n``` {.python .cell-code}\nfunctions.imshow(\n    torchvision.utils.make_grid(\n        layer_5.squeeze(0).unsqueeze(1),\n        nrow=int(512**0.5)\n    )\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-27-output-1.png){width=393 height=415}\n:::\n:::\n\n\n## Layer 6\n\n::: {#062c5369 .cell execution_count=27}\n``` {.python .cell-code}\nfunctions.imshow(\n    torchvision.utils.make_grid(\n        layer_6.squeeze(0).unsqueeze(1),\n        nrow=int(512**0.5)\n    )\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-28-output-1.png){width=393 height=415}\n:::\n:::\n\n\n## Layer 7\n\n::: {#1e475ec5 .cell execution_count=28}\n``` {.python .cell-code}\nfunctions.imshow(\n    torchvision.utils.make_grid(\n        layer_7.squeeze(0).unsqueeze(1),\n        nrow=int(1024**0.5)\n    )\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-29-output-1.png){width=424 height=415}\n:::\n:::\n\n\n## Layer 8\n\n::: {#152d6509 .cell execution_count=29}\n``` {.python .cell-code}\nfunctions.imshow(\n    torchvision.utils.make_grid(\n        layer_8.squeeze(0).unsqueeze(1),\n        nrow=int(1024**0.5)\n    )\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-30-output-1.png){width=424 height=415}\n:::\n:::\n\n\n:::\n\nThe code to train and use this model to make predictions on custom images is available in this GitHub [repo](https://github.com/Harry2687/Gender-Prediction){target=_blank}.\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}